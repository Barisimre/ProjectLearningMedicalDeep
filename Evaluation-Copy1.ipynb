{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068436c-ef88-48dc-82cd-bd3cd0de165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import monai\n",
    "from PIL import Image\n",
    "import torch\n",
    "from monai.visualize import blend_images, matshow3d, plot_2d_or_3d_image\n",
    "from tqdm.notebook import tqdm\n",
    "from medpy.metric.binary import hd, dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cf2a1-29aa-481f-b331-60d824c540f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cursor parking space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d8d8c-e2e0-4955-8d8e-6d4341616862",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_folders = [\"data/testing/testing/\" + x + \"/\" for x in os.listdir(\"data/testing/testing/\")]\n",
    "# patient_folders = [\"data/training/\" + x + \"/\" for x in os.listdir(\"data/training/\")]\n",
    "patient_files = [[x + y[:-7] for y in os.listdir(x) if \"frame\" in y and \"gt\" not in y] for x in patient_folders]\n",
    "patient_files_flattened = [element for sublist in patient_files for element in sublist]\n",
    "\n",
    "\n",
    "images = [{'img': x} for x in patient_files_flattened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fd669-3fbc-409e-af03-798e839de76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032de188-f3e8-4769-94c9-0f160fc0e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load_nii\n",
    "class LoadNIFTI(monai.transforms.Transform):\n",
    "    \"\"\"\n",
    "    This custom Monai transform loads the data from the rib segmentation dataset.\n",
    "    Defining a custom transform is simple; just overwrite the __init__ function and __call__ function.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img_file = sample['img'] + \".nii.gz\"\n",
    "        # img_mask = sample['img'] + \"_gt.nii.gz\"\n",
    "                \n",
    "        image, img_affine, img_header = load_nii(img_file)\n",
    "        image = np.moveaxis(image, (2), (0))\n",
    "        \n",
    "        header_dict[sample['img']]=img_header\n",
    "        \n",
    "        # mask, mask_affine, mask_header = load_nii(img_mask)\n",
    "        # mask = np.moveaxis(mask, (2), (0))\n",
    "        \n",
    "        return {'img': image, 'name': sample, 'affine': img_affine}\n",
    "    \n",
    "       \n",
    "class ScaleDims(monai.transforms.Transform):\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        scaling = sample['scaling']\n",
    "        \n",
    "        return monai.transforms.Zoomd(keys=['img', 'mask'], mode=['area', 'nearest'], zoom=(scaling[3] / 10, scaling[1] / 1.5, scaling[2] / 1.5), keep_size=False)(sample)\n",
    "    \n",
    "        # img = monai.transforms.Zoomd(keys=['img', 'mask']\n",
    "class ScaleDimsInverse(monai.transforms.Transform):\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        scaling = sample['scaling']\n",
    "        \n",
    "        return monai.transforms.Zoomd(keys=['img', 'mask'], mode=['area', 'nearest'], zoom=(1 / (scaling[3] / 10), 1 / (scaling[1] / 1.5), 1 / (scaling[2] / 1.5)), keep_size=False)(sample)\n",
    "    \n",
    "    \n",
    "class FindCenter(monai.transforms.Transform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.model = monai.networks.nets.Unet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=3,\n",
    "                channels = (8, 16, 32, 64),\n",
    "                strides=(1, 1, 1),\n",
    "                num_res_units=2,\n",
    "            ).to(device)\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(\"models/trainedUNet1655808833.8215299_24.pt\"))\n",
    "        self.model.eval()\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img = sample['img']\n",
    "        with torch.no_grad():\n",
    "            out = self.model(torch.unsqueeze(torch.Tensor(img).to(device), dim=0))\n",
    "            combined = np.sum(out[0].detach().cpu().numpy(), axis=(0, 1))\n",
    "            cx, cy = ndi.center_of_mass(combined)\n",
    "            dim_height = out.shape[2]\n",
    "            sample['heart_pos'] = int(cx), int(cy)\n",
    "            return monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_size=(dim_height, 128, 128), roi_center=(dim_height // 2, int(cx), int(cy)))(sample)\n",
    "\n",
    "class FindCenterInverse(monai.transforms.Transform):\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        cx, cy = sample['heart_pos']\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b539801-6bcf-4ac0-a7de-4fcd3e4790f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for loading the dataset\n",
    "\n",
    "# add_channels_transform = monai.transforms.AddChanneld(keys=['img', 'mask'])\n",
    "# flip_transform = monai.transforms.RandFlipd(keys=['img', 'mask'], prob=1, spatial_axis=1)\n",
    "# rotate_transform = monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest'])\n",
    "\n",
    "# compose_transform = monai.transforms.Compose(\n",
    "#     [\n",
    "#         LoadNIFTI(),\n",
    "#         monai.transforms.AddChanneld(keys=['img', 'mask']),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['img', 'mask'], minv=0.0, maxv=1.0),\n",
    "#         SplitMask(),\n",
    "#         monai.transforms.Resized(keys=['img', 'mask'], spatial_size=(-1, 128, 128)),\n",
    "#         monai.transforms.SpatialPadd(keys=['img', 'mask'], spatial_size=(16, -1, -1)),\n",
    "#         monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_size=(16, 128, 128), roi_center=(8, 64, 64)),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['mask'], minv=0.0, maxv=1.0)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "compose_transform = monai.transforms.Compose(\n",
    "    [\n",
    "        LoadNIFTI(),\n",
    "        monai.transforms.AddChanneld(keys=['img']),\n",
    "        monai.transforms.ScaleIntensityd(keys=['img'], minv=0.0, maxv=1.0),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800572b-818c-4dfe-a298-c5937d2f304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 100/100 [00:00<00:00, 164.45it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dict_list = [x for x in images]\n",
    "dataset = monai.data.CacheDataset(train_dict_list, transform=compose_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03315317-3b68-49ad-a78d-0ab7713525d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = monai.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0316323-c00f-4bf7-8462-0c1a8f792296",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0901219-9d3a-4726-ad7d-dfb4c9f0404c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = monai.networks.nets.UNETR(in_channels=1, out_channels=3, img_size=(16,128,128), feature_size=32, norm_name='batch').to(device)\n",
    "model = monai.networks.nets.Unet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    channels = (8, 16, 32, 64),\n",
    "    strides=(1, 1, 1),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('models/trainedUNet1655751992.34428.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02b7ec-649d-4ab3-bd06-f4d2b2d76607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'nibabel.nifti1.Nifti1Header'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type([default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed])\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type([\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed])\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:180\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'nibabel.nifti1.Nifti1Header'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/data/utils.py:276\u001b[0m, in \u001b[0;36mlist_data_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    275\u001b[0m     key \u001b[38;5;241m=\u001b[39m k\n\u001b[0;32m--> 276\u001b[0m     ret[k] \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:178\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:178\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:180\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'nibabel.nifti1.Nifti1Header'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m      8\u001b[0m     img \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m     label \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/data/utils.py:300\u001b[0m, in \u001b[0;36mlist_data_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    294\u001b[0m         re_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCollate error on the key \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of dictionary data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     re_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMONAI hint: if your transforms intentionally create mixtures of torch Tensor and numpy ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(check its documentation).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(re_str) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'nibabel.nifti1.Nifti1Header'>"
     ]
    }
   ],
   "source": [
    "# def flatten(mask):\n",
    "#     out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "#     return out\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# for d in data_loader:\n",
    "#     img = d['img']\n",
    "#     label = d['mask']\n",
    "#     pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "#     # print(pred.shape)\n",
    "#     flatt_pred = flatten(pred[0])\n",
    "#     fixed_label = 3*(label[0][0])\n",
    "#     print(flatt_pred.shape)\n",
    "#     print(fixed_label.shape)\n",
    "    \n",
    "#     print(evaluate.metrics(flatt_pred, fixed_label, [1,1,1]))\n",
    "    \n",
    "#     # evaluate.metrics(label, pred, [1, 1, 1])\n",
    "\n",
    "    \n",
    "# #     plt.imshow(d['img'][0][0][4], cmap='gray')\n",
    "# #     plt.show()\n",
    "# #     out = torch.clamp(model(d['img'].to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "# #     o = np.concatenate((out[0][0][4], out[0][1][4], out[0][2][4]), axis=1)\n",
    "    \n",
    "# #     # a = np.expand_dims(flatten(d['mask'][0, :, 4]), axis=0)\n",
    "# #     b = np.expand_dims(flatten(out[0])[4],axis=0)\n",
    "    \n",
    "# #     # plt.imshow(a[0])\n",
    "# #     # plt.show()\n",
    "# #     plt.imshow(b[0])\n",
    "# #     plt.show()\n",
    "\n",
    "#     # print(d['img'].shape)\n",
    "#     # print(evaluate.metrics(a, b, [1, 1, 1]))\n",
    "#     # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f57c57-4d46-4a03-a201-5eb3f08d1cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "def flatten(mask):\n",
    "    out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "    return out\n",
    "\n",
    "def get_name(filename):\n",
    "    patient = filename.rpartition('data/testing/testing/')[2][:10]\n",
    "    if filename[-2:] == '01':\n",
    "        state = '_ED'\n",
    "    else:\n",
    "        state = '_ES'\n",
    "    return patient+state\n",
    "\n",
    "import evaluate\n",
    "\n",
    "for d in data_loader:\n",
    "    \n",
    "    img = d['img']\n",
    "    name = get_name(d['name']['img'][0])\n",
    "    header = header_dict[d['name']['img'][0]]\n",
    "    affine = d['affine'][0]\n",
    "    # print(affine.shape)\n",
    "    pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "    flatt_pred = flatten(pred[0])\n",
    "    \n",
    "    evaluate.save_nii(\"results/masks/\"+name, flatt_pred, affine, header)\n",
    "    \n",
    "    # print(pred.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426b5b4-6d29-432a-908b-eb3f11a1d891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd87558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import monai\n",
    "from PIL import Image\n",
    "import torch\n",
    "from monai.visualize import blend_images, matshow3d, plot_2d_or_3d_image\n",
    "from tqdm.notebook import tqdm\n",
    "from medpy.metric.binary import hd, dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b95062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cursor parking space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e97a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_folders = [\"data/testing/testing/\" + x + \"/\" for x in os.listdir(\"data/testing/testing/\")]\n",
    "# patient_folders = [\"data/training/\" + x + \"/\" for x in os.listdir(\"data/training/\")]\n",
    "patient_files = [[x + y[:-7] for y in os.listdir(x) if \"frame\" in y and \"gt\" not in y] for x in patient_folders]\n",
    "patient_files_flattened = [element for sublist in patient_files for element in sublist]\n",
    "\n",
    "\n",
    "images = [{'img': x} for x in patient_files_flattened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load_nii\n",
    "class LoadNIFTI(monai.transforms.Transform):\n",
    "    \"\"\"\n",
    "    This custom Monai transform loads the data from the rib segmentation dataset.\n",
    "    Defining a custom transform is simple; just overwrite the __init__ function and __call__ function.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img_file = sample['img'] + \".nii.gz\"\n",
    "        # img_mask = sample['img'] + \"_gt.nii.gz\"\n",
    "                \n",
    "        image, img_affine, img_header = load_nii(img_file)\n",
    "        image = np.moveaxis(image, (2), (0))\n",
    "        \n",
    "        header_dict[sample['img']]=img_header\n",
    "        \n",
    "        # mask, mask_affine, mask_header = load_nii(img_mask)\n",
    "        # mask = np.moveaxis(mask, (2), (0))\n",
    "        \n",
    "        return {'img': image, 'name': sample, 'affine': img_affine}\n",
    "    \n",
    "\n",
    "class StoreNIFTI(monai.transforms.Transform):\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        print(sample)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for loading the dataset\n",
    "\n",
    "# add_channels_transform = monai.transforms.AddChanneld(keys=['img', 'mask'])\n",
    "# flip_transform = monai.transforms.RandFlipd(keys=['img', 'mask'], prob=1, spatial_axis=1)\n",
    "# rotate_transform = monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest'])\n",
    "\n",
    "# compose_transform = monai.transforms.Compose(\n",
    "#     [\n",
    "#         LoadNIFTI(),\n",
    "#         monai.transforms.AddChanneld(keys=['img', 'mask']),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['img', 'mask'], minv=0.0, maxv=1.0),\n",
    "#         SplitMask(),\n",
    "#         monai.transforms.Resized(keys=['img', 'mask'], spatial_size=(-1, 128, 128)),\n",
    "#         monai.transforms.SpatialPadd(keys=['img', 'mask'], spatial_size=(16, -1, -1)),\n",
    "#         monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_size=(16, 128, 128), roi_center=(8, 64, 64)),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['mask'], minv=0.0, maxv=1.0)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "compose_transform = monai.transforms.Compose(\n",
    "    [\n",
    "        LoadNIFTI(),\n",
    "        monai.transforms.AddChanneld(keys=['img']),\n",
    "        monai.transforms.ScaleIntensityd(keys=['img'], minv=0.0, maxv=1.0),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e20959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 100/100 [00:00<00:00, 164.45it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dict_list = [x for x in images]\n",
    "dataset = monai.data.CacheDataset(train_dict_list, transform=compose_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56914b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = monai.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359325b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdc7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = monai.networks.nets.UNETR(in_channels=1, out_channels=3, img_size=(16,128,128), feature_size=32, norm_name='batch').to(device)\n",
    "model = monai.networks.nets.Unet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    channels = (8, 16, 32, 64),\n",
    "    strides=(1, 1, 1),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('models/trainedUNet1655751992.34428.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571b4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'nibabel.nifti1.Nifti1Header'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type([default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed])\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type([\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed])\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:180\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'nibabel.nifti1.Nifti1Header'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/data/utils.py:276\u001b[0m, in \u001b[0;36mlist_data_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    275\u001b[0m     key \u001b[38;5;241m=\u001b[39m k\n\u001b[0;32m--> 276\u001b[0m     ret[k] \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:178\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:178\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:180\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'nibabel.nifti1.Nifti1Header'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m      8\u001b[0m     img \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m     label \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/data/utils.py:300\u001b[0m, in \u001b[0;36mlist_data_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    294\u001b[0m         re_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCollate error on the key \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of dictionary data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     re_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMONAI hint: if your transforms intentionally create mixtures of torch Tensor and numpy ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(check its documentation).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(re_str) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'nibabel.nifti1.Nifti1Header'>"
     ]
    }
   ],
   "source": [
    "def flatten(mask):\n",
    "    out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "    return out\n",
    "\n",
    "import evaluate\n",
    "\n",
    "for d in data_loader:\n",
    "    img = d['img']\n",
    "    label = d['mask']\n",
    "    pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "    # print(pred.shape)\n",
    "    flatt_pred = flatten(pred[0])\n",
    "    fixed_label = 3*(label[0][0])\n",
    "    print(flatt_pred.shape)\n",
    "    print(fixed_label.shape)\n",
    "    \n",
    "    print(evaluate.metrics(flatt_pred, fixed_label, [1,1,1]))\n",
    "    \n",
    "    # evaluate.metrics(label, pred, [1, 1, 1])\n",
    "\n",
    "    \n",
    "#     plt.imshow(d['img'][0][0][4], cmap='gray')\n",
    "#     plt.show()\n",
    "#     out = torch.clamp(model(d['img'].to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "#     o = np.concatenate((out[0][0][4], out[0][1][4], out[0][2][4]), axis=1)\n",
    "    \n",
    "#     # a = np.expand_dims(flatten(d['mask'][0, :, 4]), axis=0)\n",
    "#     b = np.expand_dims(flatten(out[0])[4],axis=0)\n",
    "    \n",
    "#     # plt.imshow(a[0])\n",
    "#     # plt.show()\n",
    "#     plt.imshow(b[0])\n",
    "#     plt.show()\n",
    "\n",
    "    # print(d['img'].shape)\n",
    "    # print(evaluate.metrics(a, b, [1, 1, 1]))\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d71a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "def flatten(mask):\n",
    "    out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "    return out\n",
    "\n",
    "def get_name(filename):\n",
    "    patient = filename.rpartition('data/testing/testing/')[2][:10]\n",
    "    if filename[-2:] == '01':\n",
    "        state = '_ED'\n",
    "    else:\n",
    "        state = '_ES'\n",
    "    return patient+state\n",
    "\n",
    "import evaluate\n",
    "\n",
    "for d in data_loader:\n",
    "    \n",
    "    img = d['img']\n",
    "    name = get_name(d['name']['img'][0])\n",
    "    header = header_dict[d['name']['img'][0]]\n",
    "    affine = d['affine'][0]\n",
    "    # print(affine.shape)\n",
    "    pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "    flatt_pred = flatten(pred[0])\n",
    "    \n",
    "    evaluate.save_nii(\"results/masks/\"+name, flatt_pred, affine, header)\n",
    "    \n",
    "    # print(pred.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6584785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1e34c-be00-403e-a97d-f7f6e9f71206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d7348-52a2-4f9e-8d36-1cfdc625e128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1d51a-b9a1-47ef-8941-7efa7b76d344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
