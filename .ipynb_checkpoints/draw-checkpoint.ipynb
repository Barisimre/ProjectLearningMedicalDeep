{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52cf01a1-c666-4492-b915-0eeaf600a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import monai\n",
    "from PIL import Image\n",
    "import torch\n",
    "from monai.visualize import blend_images, matshow3d, plot_2d_or_3d_image\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.ndimage as ndi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbcca63-ecaf-4d2b-b579-7b312492f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slice_n = 4\n",
    "patient=150\n",
    "pre_images = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4a79f4-b407-4591-85e9-cbad7c4e0b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_folders = [\"data/training/\" + x + \"/\" for x in os.listdir(\"data/training\")]\n",
    "train_patients = patient_folders[:80]\n",
    "test_patients = patient_folders[80:]\n",
    "\n",
    "patient_files = [[x + y[:-7] for y in os.listdir(x) if \"frame\" in y and \"gt\" not in y] for x in train_patients]\n",
    "patient_files_flattened = [element for sublist in patient_files for element in sublist]\n",
    "\n",
    "test_patient_files = [[x + y[:-7] for y in os.listdir(x) if \"frame\" in y and \"gt\" not in y] for x in test_patients]\n",
    "test_patient_files_flattened = [element for sublist in test_patient_files for element in sublist]\n",
    "\n",
    "\n",
    "images = [{'img': x} for x in patient_files_flattened]\n",
    "test_images = [{'img': x} for x in test_patient_files_flattened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6569167-64ac-4011-b527-07b48225e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load_nii\n",
    "class LoadNIFTI(monai.transforms.Transform):\n",
    "    \"\"\"\n",
    "    This custom Monai transform loads the data from the rib segmentation dataset.\n",
    "    Defining a custom transform is simple; just overwrite the __init__ function and __call__ function.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img_file = sample['img'] + \".nii.gz\"\n",
    "        img_mask = sample['img'] + \"_gt.nii.gz\"\n",
    "        \n",
    "        image, img_affine, img_header = load_nii(img_file)\n",
    "        scale_dims = img_header['pixdim']\n",
    "\n",
    "        \n",
    "        \n",
    "        image = np.moveaxis(image, (2), (0))\n",
    "        \n",
    "        \n",
    "        \n",
    "        mask, mask_affine, mask_header = load_nii(img_mask)\n",
    "        mask = np.moveaxis(mask, (2), (0))\n",
    "        \n",
    "        return {'img': image, 'mask': mask, 'name': sample, 'scaling': scale_dims, 'original_image': image, 'original_mask':mask}\n",
    "    \n",
    "    \n",
    "class SplitMask(monai.transforms.Transform):\n",
    "    \"\"\"\n",
    "    This custom Monai transform loads the data from the rib segmentation dataset.\n",
    "    Defining a custom transform is simple; just overwrite the __init__ function and __call__ function.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        mask = sample['mask'][0]\n",
    "            \n",
    "        c2 = np.where(np.logical_and(mask > 0.2, mask < 0.5), 1.0, 0.0)\n",
    "        c3 = np.where(np.logical_and(mask > 0.5, mask < 0.8), 1.0, 0.0)\n",
    "        c4 = np.where(mask > 0.8, 1.0, 0.0)\n",
    "        sample['mask'] = np.array([c2, c3, c4])\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "class ScaleDims(monai.transforms.Transform):\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        scaling = sample['scaling']\n",
    "        \n",
    "        return monai.transforms.Zoomd(keys=['img', 'mask'], mode=['area', 'nearest'], zoom=(scaling[3] / 10, scaling[1] / 1.5, scaling[2] / 1.5), keep_size=False)(sample)\n",
    "    \n",
    "        # img = monai.transforms.Zoomd(keys=['img', 'mask']\n",
    "        \n",
    "class FindCenter(monai.transforms.Transform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.model = monai.networks.nets.Unet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=3,\n",
    "                channels = (8, 16, 32, 64),\n",
    "                strides=(1, 1, 1),\n",
    "                num_res_units=2,\n",
    "            ).to(device)\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(\"models/trainedUNet1655808833.8215299_24.pt\"))\n",
    "        self.model.eval()\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img = sample['img']\n",
    "        with torch.no_grad():\n",
    "            out = self.model(torch.unsqueeze(torch.Tensor(img).to(device), dim=0))\n",
    "            combined = np.sum(out[0].detach().cpu().numpy(), axis=(0, 1))\n",
    "            cx, cy = ndi.center_of_mass(combined)\n",
    "            dim_height = out.shape[2]\n",
    "            return monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_size=(dim_height, 128, 128), roi_center=(dim_height // 2, int(cx), int(cy)))(sample)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e20e7485-cb0b-4560-bfcc-7a304dcc3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compose_transform = monai.transforms.Compose(\n",
    "    [\n",
    "        LoadNIFTI(),\n",
    "        monai.transforms.AddChanneld(keys=['img', 'mask']),\n",
    "        monai.transforms.ScaleIntensityd(keys=['img', 'mask'], minv=0.0, maxv=1.0),\n",
    "        SplitMask(),\n",
    "        monai.transforms.ScaleIntensityd(keys=['mask'], minv=0.0, maxv=1.0),\n",
    "        ScaleDims(),\n",
    "        FindCenter(),\n",
    "        # monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest']),\n",
    "        # monai.transforms.RandZoomd(keys=['img', 'mask'], prob=0.5, mode=['area', 'nearest']),\n",
    "        # monai.transforms.RandGridDistortiond(keys=['img', 'mask'], mode=['bilinear', 'nearest']),\n",
    "        # monai.transforms.SpatialPadd(keys=['img', 'mask'], spatial_size=(16, -1, -1)),\n",
    "        # monai.transforms.RandFlipd(keys=['img', 'mask'], prob=0.5, spatial_axis=1)\n",
    "        # monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_size=(16, 128, 128), roi_center=(8, 64, 64)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = monai.transforms.Compose(\n",
    "    [\n",
    "        LoadNIFTI(),\n",
    "        monai.transforms.AddChanneld(keys=['img', 'mask']),\n",
    "        monai.transforms.ScaleIntensityd(keys=['img', 'mask'], minv=0.0, maxv=1.0),\n",
    "        SplitMask(),\n",
    "        ScaleDims(),\n",
    "        FindCenter(),\n",
    "        # monai.transforms.SpatialPadd(keys=['img', 'mask'], spatial_size=(16, -1, -1)),\n",
    "        # monai.transforms.Resized(keys=['img', 'mask'], spatial_size=(-1, 128, 128)),\n",
    "        # monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_size=(16, 128, 128), roi_center=(8, 64, 64)),\n",
    "\n",
    "        monai.transforms.ScaleIntensityd(keys=['mask'], minv=0.0, maxv=1.0)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce615e71-6e21-4971-9a90-ed724d613db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 160/160 [00:13<00:00, 11.95it/s]\n",
      "Loading dataset: 100%|██████████| 40/40 [00:03<00:00, 12.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dict_list = [x for x in images]\n",
    "dataset = monai.data.CacheDataset(train_dict_list, transform=compose_transform)\n",
    "test_dict_list = [x for x in test_images]\n",
    "test_dataset = monai.data.CacheDataset(test_dict_list, transform=test_transform)\n",
    "data_loader = monai.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "test_loader = monai.data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fefd4186-ff71-4fd7-ba2d-03969e0e0821",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 29) (3553686858.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [11]\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.title(\"Augmented image\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 29)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = monai.networks.nets.UNETR(in_channels=1, out_channels=3, img_size=(16,128,128), feature_size=32, norm_name='batch').to(device)\n",
    "model = monai.networks.nets.Unet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    channels = (8, 16, 32, 64),\n",
    "    strides=(1, 1, 1),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "def flatten(mask):\n",
    "    out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "    return torch.Tensor(out)\n",
    "\n",
    "model.load_state_dict(torch.load('models/trainedUNet1655812377.0684168_136.pt'))\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "for d in data_loader:\n",
    "    \n",
    "    plt.imshow(d['original_image'][0][slice_n])\n",
    "    plt.title(\"Original image\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(d['original_mask'][0][slice_n])\n",
    "    plt.title(\"Original ground truth\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(d['img'][0][0][slice_n])\n",
    "    plt.title(\"Augmented image\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(flatten(d['mask'][0])[slice_n])\n",
    "    plt.title(\"Augmented ground truth\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    output = flatten(torch.clamp(model(d['img'].to(device)), min=0, max=1).detach().cpu().numpy()[0])\n",
    "    plt.title(\"Predicted mask\")\n",
    "\n",
    "    plt.imshow(output[slice_n])\n",
    "    plt.show()\n",
    "    \n",
    "    break\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb9e7a-578b-43ab-846f-48edbded00f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
