{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7599ca-b28c-4f77-a1d8-8ecb56dc3e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068436c-ef88-48dc-82cd-bd3cd0de165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import monai\n",
    "from PIL import Image\n",
    "import torch\n",
    "from monai.visualize import blend_images, matshow3d, plot_2d_or_3d_image\n",
    "from tqdm.notebook import tqdm\n",
    "from medpy.metric.binary import hd, dc\n",
    "import scipy.ndimage as ndi\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cf2a1-29aa-481f-b331-60d824c540f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cursor parking space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d8d8c-e2e0-4955-8d8e-6d4341616862",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_folders = [\"data/testing/testing/\" + x + \"/\" for x in os.listdir(\"data/testing/testing/\")]\n",
    "# patient_folders = [\"data/training/\" + x + \"/\" for x in os.listdir(\"data/training/\")]\n",
    "patient_files = [[x + y[:-7] for y in os.listdir(x) if \"frame\" in y and \"gt\" not in y] for x in patient_folders]\n",
    "patient_files_flattened = [element for sublist in patient_files for element in sublist]\n",
    "\n",
    "\n",
    "images = [{'img': x} for x in patient_files_flattened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fd669-3fbc-409e-af03-798e839de76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_dict = {}\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92304ee1-1a62-4141-8aaf-bf82c49686f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/training/patient029/patient029_frame01\n",
    "# patient029_frame01\n",
    "# SAVING PATIENT patient029_frame01_ES\n",
    "\n",
    "\n",
    "\n",
    "def get_name(filename):\n",
    "    number = (filename.split(\"/\")[-1]).split(\"_\")[0][-3:]\n",
    "    frame = (filename.split(\"/\")[-1]).split(\"_\")[1][-2:]\n",
    "    res = \"\"\n",
    "    if frame == \"01\":\n",
    "        res = \"patient\"+number+\"_ED\"  \n",
    "    else:\n",
    "        res = \"patient\"+number+\"_ES\"\n",
    "        \n",
    "    return res+\".nii.gz\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032de188-f3e8-4769-94c9-0f160fc0e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load_nii\n",
    "class LoadNIFTI(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.next_id = 0\n",
    "        self.headers = {}\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img_file = sample['img'] + \".nii.gz\"\n",
    "        # img_mask = sample['img'] + \"_gt.nii.gz\"\n",
    "                \n",
    "        image, img_affine, img_header = load_nii(img_file)\n",
    "        image = np.moveaxis(image, (2), (0))\n",
    "        \n",
    "        header_dict[sample['img']]=img_header\n",
    "        \n",
    "        # mask, mask_affine, mask_header = load_nii(img_mask)\n",
    "        # mask = np.moveaxis(mask, (2), (0))\n",
    "        extra_data = {\n",
    "            'name': sample['img'], \n",
    "            'affine': img_affine, \n",
    "            'original': image,\n",
    "            'scaling': img_header['pixdim'],\n",
    "            'id': self.next_id\n",
    "        }\n",
    "        self.headers[self.next_id] = img_header\n",
    "        self.next_id += 1\n",
    "        \n",
    "        return {'img': image, 'extra_data': extra_data}\n",
    "    \n",
    "    def inverse(self, sample):\n",
    "        img_header = self.headers[sample['extra_data']['id']]\n",
    "        name = sample['extra_data']['name']\n",
    "        \n",
    "        # plt.imshow(sample['img'][0][4])\n",
    "        # plt.show()\n",
    "        # plt.imshow(sample['extra_data']['original'][4])\n",
    "        # plt.show()\n",
    "        \n",
    "        reshaped = np.round(np.moveaxis(sample['img'][0].numpy(), (0, 1, 2), (2, 0, 1))).astype(np.uint8)\n",
    "\n",
    "        img_header['datatype'] = 2\n",
    "        \n",
    "        \n",
    "        evaluate.save_nii(\"results/validation_masks/\"+get_name(name), reshaped, sample['extra_data']['affine'], img_header)\n",
    "    \n",
    "       \n",
    "class ScaleDims(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.transforms = dict()\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        scaling = sample['extra_data']['scaling']\n",
    "        transform = monai.transforms.Zoomd(keys=['img'], mode=['area'], zoom=(scaling[3] / 10, scaling[1] / 1.5, scaling[2] / 1.5), keep_size=False)\n",
    "        self.transforms[sample['extra_data']['id']] = transform\n",
    "        return transform(sample)\n",
    "    \n",
    "    def inverse(self, sample):\n",
    "        return self.transforms[sample['extra_data']['id']].inverse(sample)\n",
    "    \n",
    "class FindCenter(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.model = monai.networks.nets.Unet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=3,\n",
    "                channels = (8, 16, 32, 64),\n",
    "                strides=(1, 1, 1),\n",
    "                num_res_units=2,\n",
    "            ).to(device)\n",
    "        \n",
    "        self.transforms = dict()\n",
    "        self.model.load_state_dict(torch.load(\"models/trainedUNet1655808833.8215299_24.pt\"))\n",
    "        self.model.eval\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img = sample['img']\n",
    "        with torch.no_grad():\n",
    "            out = self.model(torch.unsqueeze(torch.Tensor(img).to(device), dim=0))\n",
    "            combined = np.sum(out[0].detach().cpu().numpy(), axis=(0, 1))\n",
    "            cx, cy = ndi.center_of_mass(combined)\n",
    "            dim_height = out.shape[2]\n",
    "            self.transforms[sample['extra_data']['id']] = monai.transforms.SpatialCropd(keys=['img'], roi_size=(dim_height, 128, 128), roi_center=(dim_height // 2, int(cx), int(cy)))\n",
    "            return self.transforms[sample['extra_data']['id']](sample)\n",
    "\n",
    "    def inverse(self, sample):\n",
    "        return self.transforms[sample['extra_data']['id']].inverse(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b539801-6bcf-4ac0-a7de-4fcd3e4790f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for loading the dataset\n",
    "\n",
    "# add_channels_transform = monai.transforms.AddChanneld(keys=['img', 'mask'])\n",
    "# flip_transform = monai.transforms.RandFlipd(keys=['img', 'mask'], prob=1, spatial_axis=1)\n",
    "# rotate_transform = monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest'])\n",
    "\n",
    "# compose_transform = monai.transforms.Compose(\n",
    "#     [\n",
    "#         LoadNIFTI(),\n",
    "#         monai.transforms.AddChanneld(keys=['img', 'mask']),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['img', 'mask'], minv=0.0, maxv=1.0),\n",
    "#         SplitMask(),\n",
    "#         monai.transforms.Resized(keys=['img', 'mask'], spatial_size=(-1, 128, 128)),\n",
    "#         monai.transforms.SpatialPadd(keys=['img', 'mask'], spatial_size=(16, -1, -1)),\n",
    "#         monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_size=(16, 128, 128), roi_center=(8, 64, 64)),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['mask'], minv=0.0, maxv=1.0)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "compose_transform = monai.transforms.Compose(\n",
    "    [\n",
    "        LoadNIFTI(),\n",
    "        monai.transforms.AddChanneld(keys=['img']),\n",
    "        monai.transforms.ScaleIntensityd(keys=['img'], minv=0.0, maxv=1.0),\n",
    "        ScaleDims(),\n",
    "        FindCenter(),\n",
    "    ]\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800572b-818c-4dfe-a298-c5937d2f304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 100/100 [00:09<00:00, 10.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dict_list = [x for x in images]\n",
    "dataset = monai.data.CacheDataset(train_dict_list, transform=compose_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03315317-3b68-49ad-a78d-0ab7713525d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = monai.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0316323-c00f-4bf7-8462-0c1a8f792296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0901219-9d3a-4726-ad7d-dfb4c9f0404c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = monai.networks.nets.UNETR(in_channels=1, out_channels=3, img_size=(16,128,128), feature_size=32, norm_name='batch').to(device)\n",
    "model = monai.networks.nets.Unet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    channels = (8, 16, 32, 64),\n",
    "    strides=(1, 1, 1),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('models/trainedUNet1655812377.0684168_136.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02b7ec-649d-4ab3-bd06-f4d2b2d76607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def flatten(mask):\n",
    "#     out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "#     return out\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# for d in data_loader:\n",
    "#     img = d['img']\n",
    "#     label = d['mask']\n",
    "#     pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "#     # print(pred.shape)\n",
    "#     flatt_pred = flatten(pred[0])\n",
    "#     fixed_label = 3*(label[0][0])\n",
    "#     print(flatt_pred.shape)\n",
    "#     print(fixed_label.shape)\n",
    "    \n",
    "#     print(evaluate.metrics(flatt_pred, fixed_label, [1,1,1]))\n",
    "    \n",
    "#     # evaluate.metrics(label, pred, [1, 1, 1])\n",
    "\n",
    "    \n",
    "# #     plt.imshow(d['img'][0][0][4], cmap='gray')\n",
    "# #     plt.show()\n",
    "# #     out = torch.clamp(model(d['img'].to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "# #     o = np.concatenate((out[0][0][4], out[0][1][4], out[0][2][4]), axis=1)\n",
    "    \n",
    "# #     # a = np.expand_dims(flatten(d['mask'][0, :, 4]), axis=0)\n",
    "# #     b = np.expand_dims(flatten(out[0])[4],axis=0)\n",
    "    \n",
    "# #     # plt.imshow(a[0])\n",
    "# #     # plt.show()\n",
    "# #     plt.imshow(b[0])\n",
    "# #     plt.show()\n",
    "\n",
    "#     # print(d['img'].shape)\n",
    "#     # print(evaluate.metrics(a, b, [1, 1, 1]))\n",
    "#     # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f57c57-4d46-4a03-a201-5eb3f08d1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mask):\n",
    "    out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "    return torch.Tensor(out)\n",
    "\n",
    "for d in data_loader:\n",
    "    \n",
    "    img = d['img']\n",
    "    # print(affine.shape)\n",
    "    pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "    flatt_pred = flatten(pred[0])\n",
    "    \n",
    "    d['img'] = torch.unsqueeze(torch.unsqueeze(flatt_pred, dim=0), dim=0)\n",
    "    monai.transforms.BatchInverseTransform(compose_transform, data_loader)(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e548b-d387-4de9-a576-564c27210fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "\n",
    "# evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8b084-0f40-4481-a837-341f29d68f2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (428,512,8) (256,216,9) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics_on_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/training/patient057/patient057_frame01_gt.nii.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/validation_masks/patient107_ED.nii.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/uni/DeepMedical/ProjectLearningMedicalDeep/evaluate.py:460\u001b[0m, in \u001b[0;36mcompute_metrics_on_files\u001b[0;34m(path_gt, path_pred)\u001b[0m\n\u001b[1;32m    458\u001b[0m name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path_gt)\n\u001b[1;32m    459\u001b[0m name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 460\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzooms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[1;32m    463\u001b[0m formatting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:>14}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>7}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>10}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>7}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>10}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>8}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>10}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>11}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/uni/DeepMedical/ProjectLearningMedicalDeep/evaluate.py:429\u001b[0m, in \u001b[0;36mmetrics\u001b[0;34m(img_gt, img_pred, voxel_size)\u001b[0m\n\u001b[1;32m    426\u001b[0m pred_c_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(pred_c_i, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Compute the Dice\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m dice \u001b[38;5;241m=\u001b[39m \u001b[43mdc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_c_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_c_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Compute volume\u001b[39;00m\n\u001b[1;32m    432\u001b[0m volpred \u001b[38;5;241m=\u001b[39m pred_c_i\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(voxel_size) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000.\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/medpy/metric/binary.py:71\u001b[0m, in \u001b[0;36mdc\u001b[0;34m(result, reference)\u001b[0m\n\u001b[1;32m     68\u001b[0m result \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39matleast_1d(result\u001b[38;5;241m.\u001b[39mastype(numpy\u001b[38;5;241m.\u001b[39mbool))\n\u001b[1;32m     69\u001b[0m reference \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39matleast_1d(reference\u001b[38;5;241m.\u001b[39mastype(numpy\u001b[38;5;241m.\u001b[39mbool))\n\u001b[0;32m---> 71\u001b[0m intersection \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mcount_nonzero(\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m)\n\u001b[1;32m     73\u001b[0m size_i1 \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mcount_nonzero(result)\n\u001b[1;32m     74\u001b[0m size_i2 \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mcount_nonzero(reference)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (428,512,8) (256,216,9) "
     ]
    }
   ],
   "source": [
    "evaluate.compute_metrics_on_files(\"data/training/patient057/patient057_frame01_gt.nii.gz\", \"results/validation_masks/patient107_ED.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8310f2-9ba0-4c51-8b39-5319ebfd96f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9315255076770679,\n",
       " 8.405,\n",
       " 0.6579999999999995,\n",
       " 0.8506687647521637,\n",
       " 6.188,\n",
       " -0.3340000000000005,\n",
       " 0.8791878172588833,\n",
       " 10.527,\n",
       " -0.6160000000000014]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1, img_affine, img_header = load_nii(\"results/masks/patient029_ED.nii.gz\")\n",
    "image2, img_affine, img_header = load_nii(\"data/training/patient029/patient029_frame01_gt.nii.gz\")\n",
    "\n",
    "print(np.max(image1), np.max(image2))\n",
    "print(image1.dtype)\n",
    "evaluate.metrics(image1, image2, (1, 1, 1))\n",
    "# print(np.max(image1), np.max(image2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e03626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import monai\n",
    "from PIL import Image\n",
    "import torch\n",
    "from monai.visualize import blend_images, matshow3d, plot_2d_or_3d_image\n",
    "from tqdm.notebook import tqdm\n",
    "from medpy.metric.binary import hd, dc\n",
    "import scipy.ndimage as ndi\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cursor parking space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_folders = [\"data/testing/testing/\" + x + \"/\" for x in os.listdir(\"data/testing/testing/\")]\n",
    "# patient_folders = [\"data/training/\" + x + \"/\" for x in os.listdir(\"data/training/\")]\n",
    "patient_files = [[x + y[:-7] for y in os.listdir(x) if \"frame\" in y and \"gt\" not in y] for x in patient_folders]\n",
    "patient_files_flattened = [element for sublist in patient_files for element in sublist]\n",
    "\n",
    "\n",
    "images = [{'img': x} for x in patient_files_flattened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae706bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_dict = {}\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/training/patient029/patient029_frame01\n",
    "# patient029_frame01\n",
    "# SAVING PATIENT patient029_frame01_ES\n",
    "\n",
    "\n",
    "\n",
    "def get_name(filename):\n",
    "    number = (filename.split(\"/\")[-1]).split(\"_\")[0][-3:]\n",
    "    frame = (filename.split(\"/\")[-1]).split(\"_\")[1][-2:]\n",
    "    res = \"\"\n",
    "    if frame == \"01\":\n",
    "        res = \"patient\"+number+\"_ED\"  \n",
    "    else:\n",
    "        res = \"patient\"+number+\"_ES\"\n",
    "        \n",
    "    return res+\".nii.gz\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load_nii\n",
    "class LoadNIFTI(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.next_id = 0\n",
    "        self.headers = {}\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img_file = sample['img'] + \".nii.gz\"\n",
    "        # img_mask = sample['img'] + \"_gt.nii.gz\"\n",
    "                \n",
    "        image, img_affine, img_header = load_nii(img_file)\n",
    "        image = np.moveaxis(image, (2), (0))\n",
    "        \n",
    "        header_dict[sample['img']]=img_header\n",
    "        \n",
    "        # mask, mask_affine, mask_header = load_nii(img_mask)\n",
    "        # mask = np.moveaxis(mask, (2), (0))\n",
    "        extra_data = {\n",
    "            'name': sample['img'], \n",
    "            'affine': img_affine, \n",
    "            'original': image,\n",
    "            'scaling': img_header['pixdim'],\n",
    "            'id': self.next_id\n",
    "        }\n",
    "        self.headers[self.next_id] = img_header\n",
    "        self.next_id += 1\n",
    "        \n",
    "        return {'img': image, 'extra_data': extra_data}\n",
    "    \n",
    "    def inverse(self, sample):\n",
    "        img_header = self.headers[sample['extra_data']['id']]\n",
    "        name = sample['extra_data']['name']\n",
    "        \n",
    "        # plt.imshow(sample['img'][0][4])\n",
    "        # plt.show()\n",
    "        # plt.imshow(sample['extra_data']['original'][4])\n",
    "        # plt.show()\n",
    "        \n",
    "        reshaped = np.round(np.moveaxis(sample['img'][0].numpy(), (0, 1, 2), (2, 0, 1))).astype(np.uint8)\n",
    "\n",
    "        img_header['datatype'] = 2\n",
    "        \n",
    "        \n",
    "        evaluate.save_nii(\"results/validation_masks/\"+get_name(name), reshaped, sample['extra_data']['affine'], img_header)\n",
    "    \n",
    "       \n",
    "class ScaleDims(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.transforms = dict()\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        scaling = sample['extra_data']['scaling']\n",
    "        transform = monai.transforms.Zoomd(keys=['img'], mode=['area'], zoom=(scaling[3] / 10, scaling[1] / 1.5, scaling[2] / 1.5), keep_size=False)\n",
    "        self.transforms[sample['extra_data']['id']] = transform\n",
    "        return transform(sample)\n",
    "    \n",
    "    def inverse(self, sample):\n",
    "        return self.transforms[sample['extra_data']['id']].inverse(sample)\n",
    "    \n",
    "class FindCenter(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.model = monai.networks.nets.Unet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=3,\n",
    "                channels = (8, 16, 32, 64),\n",
    "                strides=(1, 1, 1),\n",
    "                num_res_units=2,\n",
    "            ).to(device)\n",
    "        \n",
    "        self.transforms = dict()\n",
    "        self.model.load_state_dict(torch.load(\"models/trainedUNet1655808833.8215299_24.pt\"))\n",
    "        self.model.eval\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img = sample['img']\n",
    "        with torch.no_grad():\n",
    "            out = self.model(torch.unsqueeze(torch.Tensor(img).to(device), dim=0))\n",
    "            combined = np.sum(out[0].detach().cpu().numpy(), axis=(0, 1))\n",
    "            cx, cy = ndi.center_of_mass(combined)\n",
    "            dim_height = out.shape[2]\n",
    "            self.transforms[sample['extra_data']['id']] = monai.transforms.SpatialCropd(keys=['img'], roi_size=(dim_height, 128, 128), roi_center=(dim_height // 2, int(cx), int(cy)))\n",
    "            return self.transforms[sample['extra_data']['id']](sample)\n",
    "\n",
    "    def inverse(self, sample):\n",
    "        return self.transforms[sample['extra_data']['id']].inverse(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for loading the dataset\n",
    "\n",
    "# add_channels_transform = monai.transforms.AddChanneld(keys=['img', 'mask'])\n",
    "# flip_transform = monai.transforms.RandFlipd(keys=['img', 'mask'], prob=1, spatial_axis=1)\n",
    "# rotate_transform = monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest'])\n",
    "\n",
    "# compose_transform = monai.transforms.Compose(\n",
    "#     [\n",
    "#         LoadNIFTI(),\n",
    "#         monai.transforms.AddChanneld(keys=['img', 'mask']),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['img', 'mask'], minv=0.0, maxv=1.0),\n",
    "#         SplitMask(),\n",
    "#         monai.transforms.Resized(keys=['img', 'mask'], spatial_size=(-1, 128, 128)),\n",
    "#         monai.transforms.SpatialPadd(keys=['img', 'mask'], spatial_size=(16, -1, -1)),\n",
    "#         monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_size=(16, 128, 128), roi_center=(8, 64, 64)),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['mask'], minv=0.0, maxv=1.0)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "compose_transform = monai.transforms.Compose(\n",
    "    [\n",
    "        LoadNIFTI(),\n",
    "        monai.transforms.AddChanneld(keys=['img']),\n",
    "        monai.transforms.ScaleIntensityd(keys=['img'], minv=0.0, maxv=1.0),\n",
    "        ScaleDims(),\n",
    "        FindCenter(),\n",
    "    ]\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95318e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 100/100 [00:09<00:00, 10.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dict_list = [x for x in images]\n",
    "dataset = monai.data.CacheDataset(train_dict_list, transform=compose_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = monai.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8687a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6a999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = monai.networks.nets.UNETR(in_channels=1, out_channels=3, img_size=(16,128,128), feature_size=32, norm_name='batch').to(device)\n",
    "model = monai.networks.nets.Unet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    channels = (8, 16, 32, 64),\n",
    "    strides=(1, 1, 1),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('models/trainedUNet1655812377.0684168_136.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde442a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def flatten(mask):\n",
    "#     out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "#     return out\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# for d in data_loader:\n",
    "#     img = d['img']\n",
    "#     label = d['mask']\n",
    "#     pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "#     # print(pred.shape)\n",
    "#     flatt_pred = flatten(pred[0])\n",
    "#     fixed_label = 3*(label[0][0])\n",
    "#     print(flatt_pred.shape)\n",
    "#     print(fixed_label.shape)\n",
    "    \n",
    "#     print(evaluate.metrics(flatt_pred, fixed_label, [1,1,1]))\n",
    "    \n",
    "#     # evaluate.metrics(label, pred, [1, 1, 1])\n",
    "\n",
    "    \n",
    "# #     plt.imshow(d['img'][0][0][4], cmap='gray')\n",
    "# #     plt.show()\n",
    "# #     out = torch.clamp(model(d['img'].to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "# #     o = np.concatenate((out[0][0][4], out[0][1][4], out[0][2][4]), axis=1)\n",
    "    \n",
    "# #     # a = np.expand_dims(flatten(d['mask'][0, :, 4]), axis=0)\n",
    "# #     b = np.expand_dims(flatten(out[0])[4],axis=0)\n",
    "    \n",
    "# #     # plt.imshow(a[0])\n",
    "# #     # plt.show()\n",
    "# #     plt.imshow(b[0])\n",
    "# #     plt.show()\n",
    "\n",
    "#     # print(d['img'].shape)\n",
    "#     # print(evaluate.metrics(a, b, [1, 1, 1]))\n",
    "#     # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b859c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mask):\n",
    "    out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "    return torch.Tensor(out)\n",
    "\n",
    "for d in data_loader:\n",
    "    \n",
    "    img = d['img']\n",
    "    # print(affine.shape)\n",
    "    pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "    flatt_pred = flatten(pred[0])\n",
    "    \n",
    "    d['img'] = torch.unsqueeze(torch.unsqueeze(flatt_pred, dim=0), dim=0)\n",
    "    monai.transforms.BatchInverseTransform(compose_transform, data_loader)(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3283687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "\n",
    "# evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee2a83",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (428,512,8) (256,216,9) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics_on_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/training/patient057/patient057_frame01_gt.nii.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/validation_masks/patient107_ED.nii.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/uni/DeepMedical/ProjectLearningMedicalDeep/evaluate.py:460\u001b[0m, in \u001b[0;36mcompute_metrics_on_files\u001b[0;34m(path_gt, path_pred)\u001b[0m\n\u001b[1;32m    458\u001b[0m name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path_gt)\n\u001b[1;32m    459\u001b[0m name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 460\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzooms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[1;32m    463\u001b[0m formatting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:>14}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>7}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>10}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>7}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>10}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>8}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>10}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:>11}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/uni/DeepMedical/ProjectLearningMedicalDeep/evaluate.py:429\u001b[0m, in \u001b[0;36mmetrics\u001b[0;34m(img_gt, img_pred, voxel_size)\u001b[0m\n\u001b[1;32m    426\u001b[0m pred_c_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(pred_c_i, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Compute the Dice\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m dice \u001b[38;5;241m=\u001b[39m \u001b[43mdc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_c_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_c_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Compute volume\u001b[39;00m\n\u001b[1;32m    432\u001b[0m volpred \u001b[38;5;241m=\u001b[39m pred_c_i\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(voxel_size) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000.\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/medpy/metric/binary.py:71\u001b[0m, in \u001b[0;36mdc\u001b[0;34m(result, reference)\u001b[0m\n\u001b[1;32m     68\u001b[0m result \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39matleast_1d(result\u001b[38;5;241m.\u001b[39mastype(numpy\u001b[38;5;241m.\u001b[39mbool))\n\u001b[1;32m     69\u001b[0m reference \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39matleast_1d(reference\u001b[38;5;241m.\u001b[39mastype(numpy\u001b[38;5;241m.\u001b[39mbool))\n\u001b[0;32m---> 71\u001b[0m intersection \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mcount_nonzero(\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m)\n\u001b[1;32m     73\u001b[0m size_i1 \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mcount_nonzero(result)\n\u001b[1;32m     74\u001b[0m size_i2 \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mcount_nonzero(reference)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (428,512,8) (256,216,9) "
     ]
    }
   ],
   "source": [
    "evaluate.compute_metrics_on_files(\"data/training/patient057/patient057_frame01_gt.nii.gz\", \"results/validation_masks/patient107_ED.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9315255076770679,\n",
       " 8.405,\n",
       " 0.6579999999999995,\n",
       " 0.8506687647521637,\n",
       " 6.188,\n",
       " -0.3340000000000005,\n",
       " 0.8791878172588833,\n",
       " 10.527,\n",
       " -0.6160000000000014]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1, img_affine, img_header = load_nii(\"results/masks/patient029_ED.nii.gz\")\n",
    "image2, img_affine, img_header = load_nii(\"data/training/patient029/patient029_frame01_gt.nii.gz\")\n",
    "\n",
    "print(np.max(image1), np.max(image2))\n",
    "print(image1.dtype)\n",
    "evaluate.metrics(image1, image2, (1, 1, 1))\n",
    "# print(np.max(image1), np.max(image2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
