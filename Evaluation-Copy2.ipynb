{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068436c-ef88-48dc-82cd-bd3cd0de165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import monai\n",
    "from PIL import Image\n",
    "import torch\n",
    "from monai.visualize import blend_images, matshow3d, plot_2d_or_3d_image\n",
    "from tqdm.notebook import tqdm\n",
    "from medpy.metric.binary import hd, dc\n",
    "import scipy.ndimage as ndi\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cf2a1-29aa-481f-b331-60d824c540f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cursor parking space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d8d8c-e2e0-4955-8d8e-6d4341616862",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_folders = [\"data/testing/testing/\" + x + \"/\" for x in os.listdir(\"data/testing/testing/\")]\n",
    "# patient_folders = [\"data/training/\" + x + \"/\" for x in os.listdir(\"data/training/\")]\n",
    "patient_files = [[x + y[:-7] for y in os.listdir(x) if \"frame\" in y and \"gt\" not in y] for x in patient_folders]\n",
    "patient_files_flattened = [element for sublist in patient_files for element in sublist]\n",
    "\n",
    "\n",
    "images = [{'img': x} for x in patient_files_flattened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fd669-3fbc-409e-af03-798e839de76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_dict = {}\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92304ee1-1a62-4141-8aaf-bf82c49686f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/training/patient029/patient029_frame01\n",
    "# patient029_frame01\n",
    "# SAVING PATIENT patient029_frame01_ES\n",
    "\n",
    "\n",
    "\n",
    "def get_name(filename):\n",
    "    number = (filename.split(\"/\")[-1]).split(\"_\")[0][-3:]\n",
    "    frame = (filename.split(\"/\")[-1]).split(\"_\")[1][-2:]\n",
    "    print(frame)\n",
    "    res = \"\"\n",
    "    if frame == \"01\":\n",
    "        res = \"patient\"+number+\"_ED\"  \n",
    "    else:\n",
    "        res = \"patient\"+number+\"_ES\"\n",
    "        \n",
    "    return res+\".nii.gz\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032de188-f3e8-4769-94c9-0f160fc0e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load_nii\n",
    "class LoadNIFTI(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.next_id = 0\n",
    "        self.headers = {}\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img_file = sample['img'] + \".nii.gz\"\n",
    "        # img_mask = sample['img'] + \"_gt.nii.gz\"\n",
    "                \n",
    "        image, img_affine, img_header = load_nii(img_file)\n",
    "        image = np.moveaxis(image, (2), (0))\n",
    "        \n",
    "        header_dict[sample['img']]=img_header\n",
    "        \n",
    "        # mask, mask_affine, mask_header = load_nii(img_mask)\n",
    "        # mask = np.moveaxis(mask, (2), (0))\n",
    "        extra_data = {\n",
    "            'name': sample['img'], \n",
    "            'affine': img_affine, \n",
    "            'original': image,\n",
    "            'scaling': img_header['pixdim'],\n",
    "            'id': self.next_id\n",
    "        }\n",
    "        self.headers[self.next_id] = img_header\n",
    "        self.next_id += 1\n",
    "        \n",
    "        return {'img': image, 'extra_data': extra_data}\n",
    "    \n",
    "    def inverse(self, sample):\n",
    "        img_header = self.headers[sample['extra_data']['id']]\n",
    "        name = sample['extra_data']['name']\n",
    "        print(\"SAVING PATIENT\", get_name(name))\n",
    "        print(\"SHAPIE\", sample['img'][0].shape)\n",
    "        \n",
    "        plt.imshow(sample['img'][0][4])\n",
    "        plt.show()\n",
    "        plt.imshow(sample['extra_data']['original'][4])\n",
    "        plt.show()\n",
    "        \n",
    "        reshaped = np.round(np.moveaxis(sample['img'][0].numpy(), (0, 1, 2), (2, 0, 1))).astype(np.uint8)\n",
    "        print(\"SHAPIE2\", reshaped.shape)\n",
    "        \n",
    "        print(img_header['datatype'])\n",
    "        print(type(img_header['datatype']))\n",
    "        img_header['datatype'] = 2\n",
    "        \n",
    "        \n",
    "        evaluate.save_nii(\"results/validation_masks/\"+get_name(name), reshaped, sample['extra_data']['affine'], img_header)\n",
    "    \n",
    "       \n",
    "class ScaleDims(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.transforms = dict()\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        scaling = sample['extra_data']['scaling']\n",
    "        transform = monai.transforms.Zoomd(keys=['img'], mode=['area'], zoom=(scaling[3] / 10, scaling[1] / 1.5, scaling[2] / 1.5), keep_size=False)\n",
    "        self.transforms[sample['extra_data']['id']] = transform\n",
    "        return transform(sample)\n",
    "    \n",
    "    def inverse(self, sample):\n",
    "        print(\"INVERSING SCALEDIMS\")\n",
    "        return self.transforms[sample['extra_data']['id']].inverse(sample)\n",
    "    \n",
    "class FindCenter(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.model = monai.networks.nets.Unet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=3,\n",
    "                channels = (8, 16, 32, 64),\n",
    "                strides=(1, 1, 1),\n",
    "                num_res_units=2,\n",
    "            ).to(device)\n",
    "        \n",
    "        self.transforms = dict()\n",
    "        self.model.load_state_dict(torch.load(\"models/trainedUNet1655808833.8215299_24.pt\"))\n",
    "        self.model.eval\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img = sample['img']\n",
    "        print(img.shape)\n",
    "        with torch.no_grad():\n",
    "            out = self.model(torch.unsqueeze(torch.Tensor(img).to(device), dim=0))\n",
    "            combined = np.sum(out[0].detach().cpu().numpy(), axis=(0, 1))\n",
    "            cx, cy = ndi.center_of_mass(combined)\n",
    "            dim_height = out.shape[2]\n",
    "            self.transforms[sample['extra_data']['id']] = monai.transforms.SpatialCropd(keys=['img'], roi_size=(dim_height, 128, 128), roi_center=(dim_height // 2, int(cx), int(cy)))\n",
    "            return self.transforms[sample['extra_data']['id']](sample)\n",
    "\n",
    "    def inverse(self, sample):\n",
    "        print(\"FIND CENTER INVERSE\")\n",
    "        return self.transforms[sample['extra_data']['id']].inverse(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b539801-6bcf-4ac0-a7de-4fcd3e4790f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for loading the dataset\n",
    "\n",
    "# add_channels_transform = monai.transforms.AddChanneld(keys=['img', 'mask'])\n",
    "# flip_transform = monai.transforms.RandFlipd(keys=['img', 'mask'], prob=1, spatial_axis=1)\n",
    "# rotate_transform = monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest'])\n",
    "\n",
    "# compose_transform = monai.transforms.Compose(\n",
    "#     [\n",
    "#         LoadNIFTI(),\n",
    "#         monai.transforms.AddChanneld(keys=['img', 'mask']),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['img', 'mask'], minv=0.0, maxv=1.0),\n",
    "#         SplitMask(),\n",
    "#         monai.transforms.Resized(keys=['img', 'mask'], spatial_size=(-1, 128, 128)),\n",
    "#         monai.transforms.SpatialPadd(keys=['img', 'mask'], spatial_size=(16, -1, -1)),\n",
    "#         monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_size=(16, 128, 128), roi_center=(8, 64, 64)),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['mask'], minv=0.0, maxv=1.0)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "compose_transform = monai.transforms.Compose(\n",
    "    [\n",
    "        LoadNIFTI(),\n",
    "        monai.transforms.AddChanneld(keys=['img']),\n",
    "        monai.transforms.ScaleIntensityd(keys=['img'], minv=0.0, maxv=1.0),\n",
    "        ScaleDims(),\n",
    "        FindCenter(),\n",
    "    ]\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800572b-818c-4dfe-a298-c5937d2f304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict_list = [x for x in images]\n",
    "dataset = monai.data.CacheDataset(train_dict_list, transform=compose_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03315317-3b68-49ad-a78d-0ab7713525d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = monai.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0316323-c00f-4bf7-8462-0c1a8f792296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0901219-9d3a-4726-ad7d-dfb4c9f0404c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = monai.networks.nets.UNETR(in_channels=1, out_channels=3, img_size=(16,128,128), feature_size=32, norm_name='batch').to(device)\n",
    "model = monai.networks.nets.Unet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    channels = (8, 16, 32, 64),\n",
    "    strides=(1, 1, 1),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('models/trainedUNet1655812377.0684168_136.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02b7ec-649d-4ab3-bd06-f4d2b2d76607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def flatten(mask):\n",
    "#     out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "#     return out\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# for d in data_loader:\n",
    "#     img = d['img']\n",
    "#     label = d['mask']\n",
    "#     pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "#     # print(pred.shape)\n",
    "#     flatt_pred = flatten(pred[0])\n",
    "#     fixed_label = 3*(label[0][0])\n",
    "#     print(flatt_pred.shape)\n",
    "#     print(fixed_label.shape)\n",
    "    \n",
    "#     print(evaluate.metrics(flatt_pred, fixed_label, [1,1,1]))\n",
    "    \n",
    "#     # evaluate.metrics(label, pred, [1, 1, 1])\n",
    "\n",
    "    \n",
    "# #     plt.imshow(d['img'][0][0][4], cmap='gray')\n",
    "# #     plt.show()\n",
    "# #     out = torch.clamp(model(d['img'].to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "# #     o = np.concatenate((out[0][0][4], out[0][1][4], out[0][2][4]), axis=1)\n",
    "    \n",
    "# #     # a = np.expand_dims(flatten(d['mask'][0, :, 4]), axis=0)\n",
    "# #     b = np.expand_dims(flatten(out[0])[4],axis=0)\n",
    "    \n",
    "# #     # plt.imshow(a[0])\n",
    "# #     # plt.show()\n",
    "# #     plt.imshow(b[0])\n",
    "# #     plt.show()\n",
    "\n",
    "#     # print(d['img'].shape)\n",
    "#     # print(evaluate.metrics(a, b, [1, 1, 1]))\n",
    "#     # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f57c57-4d46-4a03-a201-5eb3f08d1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mask):\n",
    "    out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "    return torch.Tensor(out)\n",
    "\n",
    "for d in data_loader:\n",
    "    \n",
    "    img = d['img']\n",
    "    # print(affine.shape)\n",
    "    pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "    flatt_pred = flatten(pred[0])\n",
    "    \n",
    "    d['img'] = torch.unsqueeze(torch.unsqueeze(flatt_pred, dim=0), dim=0)\n",
    "    monai.transforms.BatchInverseTransform(compose_transform, data_loader)(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e548b-d387-4de9-a576-564c27210fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "\n",
    "# evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8b084-0f40-4481-a837-341f29d68f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.compute_metrics_on_files(\"data/training/patient057/patient057_frame01_gt.nii.gz\", \"results/masks/patient057_ED.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8310f2-9ba0-4c51-8b39-5319ebfd96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1, img_affine, img_header = load_nii(\"results/masks/patient029_ED.nii.gz\")\n",
    "image2, img_affine, img_header = load_nii(\"data/training/patient029/patient029_frame01_gt.nii.gz\")\n",
    "\n",
    "print(np.max(image1), np.max(image2))\n",
    "print(image1.dtype)\n",
    "evaluate.metrics(image1, image2, (1, 1, 1))\n",
    "# print(np.max(image1), np.max(image2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5887c07e-22ee-4c53-93ff-6f0275d97821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d3b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import monai\n",
    "from PIL import Image\n",
    "import torch\n",
    "from monai.visualize import blend_images, matshow3d, plot_2d_or_3d_image\n",
    "from tqdm.notebook import tqdm\n",
    "from medpy.metric.binary import hd, dc\n",
    "import scipy.ndimage as ndi\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cursor parking space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab6555",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_folders = [\"data/testing/testing/\" + x + \"/\" for x in os.listdir(\"data/testing/testing/\")]\n",
    "# patient_folders = [\"data/training/\" + x + \"/\" for x in os.listdir(\"data/training/\")]\n",
    "patient_files = [[x + y[:-7] for y in os.listdir(x) if \"frame\" in y and \"gt\" not in y] for x in patient_folders]\n",
    "patient_files_flattened = [element for sublist in patient_files for element in sublist]\n",
    "\n",
    "\n",
    "images = [{'img': x} for x in patient_files_flattened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_dict = {}\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeefc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/training/patient029/patient029_frame01\n",
    "# patient029_frame01\n",
    "# SAVING PATIENT patient029_frame01_ES\n",
    "\n",
    "\n",
    "\n",
    "def get_name(filename):\n",
    "    number = (filename.split(\"/\")[-1]).split(\"_\")[0][-3:]\n",
    "    frame = (filename.split(\"/\")[-1]).split(\"_\")[1][-2:]\n",
    "    print(frame)\n",
    "    res = \"\"\n",
    "    if frame == \"01\":\n",
    "        res = \"patient\"+number+\"_ED\"  \n",
    "    else:\n",
    "        res = \"patient\"+number+\"_ES\"\n",
    "        \n",
    "    return res+\".nii.gz\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load_nii\n",
    "class LoadNIFTI(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.next_id = 0\n",
    "        self.headers = {}\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img_file = sample['img'] + \".nii.gz\"\n",
    "        # img_mask = sample['img'] + \"_gt.nii.gz\"\n",
    "                \n",
    "        image, img_affine, img_header = load_nii(img_file)\n",
    "        image = np.moveaxis(image, (2), (0))\n",
    "        \n",
    "        header_dict[sample['img']]=img_header\n",
    "        \n",
    "        # mask, mask_affine, mask_header = load_nii(img_mask)\n",
    "        # mask = np.moveaxis(mask, (2), (0))\n",
    "        extra_data = {\n",
    "            'name': sample['img'], \n",
    "            'affine': img_affine, \n",
    "            'original': image,\n",
    "            'scaling': img_header['pixdim'],\n",
    "            'id': self.next_id\n",
    "        }\n",
    "        self.headers[self.next_id] = img_header\n",
    "        self.next_id += 1\n",
    "        \n",
    "        return {'img': image, 'extra_data': extra_data}\n",
    "    \n",
    "    def inverse(self, sample):\n",
    "        img_header = self.headers[sample['extra_data']['id']]\n",
    "        name = sample['extra_data']['name']\n",
    "        print(\"SAVING PATIENT\", get_name(name))\n",
    "        print(\"SHAPIE\", sample['img'][0].shape)\n",
    "        \n",
    "        plt.imshow(sample['img'][0][4])\n",
    "        plt.show()\n",
    "        plt.imshow(sample['extra_data']['original'][4])\n",
    "        plt.show()\n",
    "        \n",
    "        reshaped = np.round(np.moveaxis(sample['img'][0].numpy(), (0, 1, 2), (2, 0, 1))).astype(np.uint8)\n",
    "        print(\"SHAPIE2\", reshaped.shape)\n",
    "        \n",
    "        print(img_header['datatype'])\n",
    "        print(type(img_header['datatype']))\n",
    "        img_header['datatype'] = 2\n",
    "        \n",
    "        \n",
    "        evaluate.save_nii(\"results/validation_masks/\"+get_name(name), reshaped, sample['extra_data']['affine'], img_header)\n",
    "    \n",
    "       \n",
    "class ScaleDims(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.transforms = dict()\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        scaling = sample['extra_data']['scaling']\n",
    "        transform = monai.transforms.Zoomd(keys=['img'], mode=['area'], zoom=(scaling[3] / 10, scaling[1] / 1.5, scaling[2] / 1.5), keep_size=False)\n",
    "        self.transforms[sample['extra_data']['id']] = transform\n",
    "        return transform(sample)\n",
    "    \n",
    "    def inverse(self, sample):\n",
    "        print(\"INVERSING SCALEDIMS\")\n",
    "        return self.transforms[sample['extra_data']['id']].inverse(sample)\n",
    "    \n",
    "class FindCenter(monai.transforms.InvertibleTransform):\n",
    "    def __init__(self, keys=None):\n",
    "        self.model = monai.networks.nets.Unet(\n",
    "                spatial_dims=3,\n",
    "                in_channels=1,\n",
    "                out_channels=3,\n",
    "                channels = (8, 16, 32, 64),\n",
    "                strides=(1, 1, 1),\n",
    "                num_res_units=2,\n",
    "            ).to(device)\n",
    "        \n",
    "        self.transforms = dict()\n",
    "        self.model.load_state_dict(torch.load(\"models/trainedUNet1655808833.8215299_24.pt\"))\n",
    "        self.model.eval\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img = sample['img']\n",
    "        print(img.shape)\n",
    "        with torch.no_grad():\n",
    "            out = self.model(torch.unsqueeze(torch.Tensor(img).to(device), dim=0))\n",
    "            combined = np.sum(out[0].detach().cpu().numpy(), axis=(0, 1))\n",
    "            cx, cy = ndi.center_of_mass(combined)\n",
    "            dim_height = out.shape[2]\n",
    "            self.transforms[sample['extra_data']['id']] = monai.transforms.SpatialCropd(keys=['img'], roi_size=(dim_height, 128, 128), roi_center=(dim_height // 2, int(cx), int(cy)))\n",
    "            return self.transforms[sample['extra_data']['id']](sample)\n",
    "\n",
    "    def inverse(self, sample):\n",
    "        print(\"FIND CENTER INVERSE\")\n",
    "        return self.transforms[sample['extra_data']['id']].inverse(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92400984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for loading the dataset\n",
    "\n",
    "# add_channels_transform = monai.transforms.AddChanneld(keys=['img', 'mask'])\n",
    "# flip_transform = monai.transforms.RandFlipd(keys=['img', 'mask'], prob=1, spatial_axis=1)\n",
    "# rotate_transform = monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest'])\n",
    "\n",
    "# compose_transform = monai.transforms.Compose(\n",
    "#     [\n",
    "#         LoadNIFTI(),\n",
    "#         monai.transforms.AddChanneld(keys=['img', 'mask']),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['img', 'mask'], minv=0.0, maxv=1.0),\n",
    "#         SplitMask(),\n",
    "#         monai.transforms.Resized(keys=['img', 'mask'], spatial_size=(-1, 128, 128)),\n",
    "#         monai.transforms.SpatialPadd(keys=['img', 'mask'], spatial_size=(16, -1, -1)),\n",
    "#         monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_size=(16, 128, 128), roi_center=(8, 64, 64)),\n",
    "#         monai.transforms.ScaleIntensityd(keys=['mask'], minv=0.0, maxv=1.0)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "compose_transform = monai.transforms.Compose(\n",
    "    [\n",
    "        LoadNIFTI(),\n",
    "        monai.transforms.AddChanneld(keys=['img']),\n",
    "        monai.transforms.ScaleIntensityd(keys=['img'], minv=0.0, maxv=1.0),\n",
    "        ScaleDims(),\n",
    "        FindCenter(),\n",
    "    ]\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict_list = [x for x in images]\n",
    "dataset = monai.data.CacheDataset(train_dict_list, transform=compose_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca11a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = monai.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82dfe20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4970d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = monai.networks.nets.UNETR(in_channels=1, out_channels=3, img_size=(16,128,128), feature_size=32, norm_name='batch').to(device)\n",
    "model = monai.networks.nets.Unet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    channels = (8, 16, 32, 64),\n",
    "    strides=(1, 1, 1),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('models/trainedUNet1655812377.0684168_136.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc50746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def flatten(mask):\n",
    "#     out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "#     return out\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# for d in data_loader:\n",
    "#     img = d['img']\n",
    "#     label = d['mask']\n",
    "#     pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "#     # print(pred.shape)\n",
    "#     flatt_pred = flatten(pred[0])\n",
    "#     fixed_label = 3*(label[0][0])\n",
    "#     print(flatt_pred.shape)\n",
    "#     print(fixed_label.shape)\n",
    "    \n",
    "#     print(evaluate.metrics(flatt_pred, fixed_label, [1,1,1]))\n",
    "    \n",
    "#     # evaluate.metrics(label, pred, [1, 1, 1])\n",
    "\n",
    "    \n",
    "# #     plt.imshow(d['img'][0][0][4], cmap='gray')\n",
    "# #     plt.show()\n",
    "# #     out = torch.clamp(model(d['img'].to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "# #     o = np.concatenate((out[0][0][4], out[0][1][4], out[0][2][4]), axis=1)\n",
    "    \n",
    "# #     # a = np.expand_dims(flatten(d['mask'][0, :, 4]), axis=0)\n",
    "# #     b = np.expand_dims(flatten(out[0])[4],axis=0)\n",
    "    \n",
    "# #     # plt.imshow(a[0])\n",
    "# #     # plt.show()\n",
    "# #     plt.imshow(b[0])\n",
    "# #     plt.show()\n",
    "\n",
    "#     # print(d['img'].shape)\n",
    "#     # print(evaluate.metrics(a, b, [1, 1, 1]))\n",
    "#     # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mask):\n",
    "    out = np.where(mask[2] >=0.5 , 3, np.where(mask[1] >= 0.5, 2, np.where(mask[0]>=0.5, 1, 0)))\n",
    "    return torch.Tensor(out)\n",
    "\n",
    "for d in data_loader:\n",
    "    \n",
    "    img = d['img']\n",
    "    # print(affine.shape)\n",
    "    pred = torch.clamp(model(img.to(device)), min=0, max=1).detach().cpu().numpy()\n",
    "    \n",
    "    flatt_pred = flatten(pred[0])\n",
    "    \n",
    "    d['img'] = torch.unsqueeze(torch.unsqueeze(flatt_pred, dim=0), dim=0)\n",
    "    monai.transforms.BatchInverseTransform(compose_transform, data_loader)(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "\n",
    "# evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.compute_metrics_on_files(\"data/training/patient057/patient057_frame01_gt.nii.gz\", \"results/masks/patient057_ED.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1, img_affine, img_header = load_nii(\"results/masks/patient029_ED.nii.gz\")\n",
    "image2, img_affine, img_header = load_nii(\"data/training/patient029/patient029_frame01_gt.nii.gz\")\n",
    "\n",
    "print(np.max(image1), np.max(image2))\n",
    "print(image1.dtype)\n",
    "evaluate.metrics(image1, image2, (1, 1, 1))\n",
    "# print(np.max(image1), np.max(image2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a721f443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
